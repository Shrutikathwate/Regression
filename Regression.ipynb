{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Regression"
      ],
      "metadata": {
        "id": "GLOHFFmAO30X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Simple Linear Regression?\n",
        "   - Simple Linear Regression is a statistical and machine learning method used to understand and model the relationship between one independent variable (X) and one dependent variable (Y) by fitting a straight line to the data.\n",
        "   - It assumes that the dependent variable changes linearly with the independent variable.\n",
        "   - Mathematical Equation: Y=mX+c\n",
        "\n",
        "2. What are the key assumptions of Simple Linear Regression?\n",
        "   - What are the key assumptions of Simple Linear Regression\n",
        "   - The residuals (errors) should be independent of each other.\n",
        "   - The variance of errors should be constant for all values of X.\n",
        "   - The residuals should be normally distributed.\n",
        "   - Since there is only one independent variable, multicollinearity does not occur.\n",
        "   - The average of residuals should be zero.\n",
        "\n",
        "3. What does the coefficient m represent in the equation Y=mX+c?\n",
        "   - It shows how much Y changes when X increases by 1 unit. It indicates the direction and strength of the relationship between X and Y.\n",
        "   - m > 0 â†’ Positive relationship, m < 0 â†’ Negative relationship, m = 0 â†’ No linear relationship\n",
        "\n",
        "4. What does the intercept c represent in the equation Y=mX+c?\n",
        "   - It is the point where the regression line crosses the Y-axis. It shows the starting or baseline value of the dependent variable.\n",
        "\n",
        "5. How do we calculate the slope m in Simple Linear Regression?\n",
        "   - Formula to Calculate Slope (m): ð‘š =âˆ‘(ð‘¥âˆ’ð‘¥Ë‰)(ð‘¦âˆ’ð‘¦Ë‰)/âˆ‘(ð‘¥âˆ’ð‘¥Ë‰)^2\n",
        "   - Slope m = covariance(X, Y) / variance(X)\n",
        "\n",
        "6. What is the purpose of the least squares method in Simple Linear Regression?\n",
        "   - To minimize the total error between actual values and predicted values. More precisely, it minimizes the sum of the squares of the residuals.\n",
        "   - The least squares method gives: Best values of m and c, Most accurate predictions under linear assumptions.\n",
        "\n",
        "7. How is the coefficient of determination (RÂ²) interpreted in Simple Linear Regression?\n",
        "   - Coefficient of Determination (RÂ²) indicates the proportion of variance in the dependent variable (Y) that is explained by the independent variable (X). Its value ranges from 0 to 1, where a higher value means a better model fit.\n",
        "\n",
        "8. What is Multiple Linear Regression\n",
        "   - Multiple Linear Regression is a statistical method used to model the relationship between one dependent variable (Y) and two or more independent variables (Xâ‚, Xâ‚‚, â€¦, Xâ‚™) by fitting a linear equation.\n",
        "   -Equation: Y=b0â€‹+b1â€‹X1â€‹+b2â€‹X2â€‹+â‹¯+bnâ€‹Xnâ€‹\n",
        "   - It helps in predicting Y and understanding the effect of each independent variable on Y while keeping others constant.\n",
        "\n",
        "9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "   - The main difference is the number of independent variables used:       1. Simple Linear Regression uses one independent variable to predict the dependent variable.\n",
        "   2. Multiple Linear Regression uses two or more independent variables to predict the dependent variable.\n",
        "\n",
        "10. What are the key assumptions of Multiple Linear Regression?\n",
        "    - Key assumptions of Multiple Linear Regression are:  \n",
        "    1. Linearity â€“ The relationship between independent variables and the dependent variable is linear.\n",
        "    2. Independence of errors â€“ Residuals are independent of each other.\n",
        "    3. Homoscedasticity â€“ Constant variance of errors across all levels of predictors.\n",
        "    4. Normality of errors â€“ Residuals are normally distributed.\n",
        "    5. No multicollinearity â€“ Independent variables should not be highly correlated.\n",
        "\n",
        "11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "    - Heteroscedasticity occurs when the variance of residuals is not constant across all levels of independent variables.\n",
        "    - Effect on Multiple Linear Regression:                               \n",
        "    1. Coefficient estimates remain unbiased\n",
        "    2. Standard errors become incorrect/\n",
        "    3. Leads to unreliable hypothesis tests and confidence intervals\n",
        "    4. Can result in wrong conclusions about variable significanc\n",
        "\n",
        "12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "    - To improve a Multiple Linear Regression model with high multicollinearity, you can:\n",
        "    1. Remove highly correlated variables to reduce redundancy.\n",
        "    2. Combine variables (e.g., using feature engineering or PCA).\n",
        "    3. Use regularization techniques like Ridge or Lasso regression.\n",
        "    4. Increase sample size to stabilize coefficient estimates.\n",
        "    5. Check VIF (Variance Inflation Factor) and drop variables with high VIF values.\n",
        "\n",
        "13. What are some common techniques for transforming categorical variables for use in regression models?\n",
        "    - Common techniques for transforming categorical variables for regression models are:\n",
        "    1. Label Encoding â€“ Assigns a unique number to each category.\n",
        "    2. One-Hot Encoding (Dummy Variables) â€“ Creates binary (0/1) columns for each category.\n",
        "    3. Ordinal Encoding â€“ Encodes categories based on their order or rank.\n",
        "    4. Binary Encoding â€“ Converts categories into binary digits (useful for high-cardinality data).\n",
        "    - These techniques convert categorical data into numerical form so it can be used in regression models.\n",
        "\n",
        "14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "    - Interaction terms in Multiple Linear Regression capture the effect of one independent variable on the dependent variable depending on the value of another independent variable.\n",
        "    - They help model situations where the combined influence of variables is not simply additive, improving model accuracy and interpretation.\n",
        "\n",
        "15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "    - In Simple Linear Regression, the intercept represents the expected value of the dependent variable when the single independent variable is zero.\n",
        "    - In Multiple Linear Regression, the intercept represents the expected value of the dependent variable when all independent variables are zero, which may be less meaningful or unrealistic in many real-world situations.\n",
        "\n",
        "16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "    - In regression analysis, the slope represents the rate of change of the dependent variable (Y) with respect to the independent variable (X)- Significance:\n",
        "    1. Indicates the direction of the relationship: Positive slope â†’ Y increases as X increases, Negative slope â†’ Y decreases as X increases\n",
        "    2. Measures the strength of influence of X on Y\n",
        "    - Effect on predictions:\n",
        "    1. Determines how much Y changes for a unit change in X\n",
        "    2. Larger absolute slope â†’ bigger impact of X on predicted Y.\n",
        "\n",
        "17. How does the intercept in a regression model provide context for the relationship between variables?\n",
        "    - The intercept in a regression model provides the baseline value of the dependent variable (Y) when all independent variables (X) are zero.- Context it provides:\n",
        "    1. Acts as a starting point for predictions.\n",
        "    2. Helps understand the value of Y without any influence from predictors.\n",
        "    3. Gives perspective on the relative contribution of each independent variable when combined with slopes.\n",
        "\n",
        "18. What are the limitations of using RÂ² as a sole measure of model performance?\n",
        "    - The limitations of using RÂ² as the sole measure of model performance are:\n",
        "    1. Does not indicate causation â€“ High RÂ² doesnâ€™t mean X causes Y.\n",
        "    2. Ignores overfitting â€“ Adding more variables can increase RÂ² even if they are irrelevant.\n",
        "    3. Doesnâ€™t reflect prediction accuracy â€“ A high RÂ² doesnâ€™t guarantee good predictions on new data.\n",
        "    4. Insensitive to model bias â€“ A model can have high RÂ² but still have biased predictions.\n",
        "    5. Not useful for non-linear relationships â€“ RÂ² assumes linearity; it may mislead if the relationship is non-linear.\n",
        "\n",
        "19. How would you interpret a large standard error for a regression coefficient?\n",
        "    - A large standard error for a regression coefficient indicates that the estimate of the coefficient is imprecise.\n",
        "    - Interpretation:\n",
        "    1. The coefficient may vary widely if the model were repeated on different samples.\n",
        "    2. It reduces the statistical significance of the coefficient (harder to reject null hypothesis).\n",
        "    3. Could suggest high variability in the data or multicollinearity among predictors.\n",
        "\n",
        "20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "    - Heteroscedasticity appears in residual plots when the spread of residuals increases or decreases with fitted values (e.g., funnel shape). Itâ€™s important to address because it violates regression assumptions, leading to inefficient estimates and unreliable hypothesis tests.\n",
        "\n",
        "21. What does it mean if a Multiple Linear Regression model has a high RÂ² but low adjusted RÂ²?\n",
        "    - It means the model explains much of the variance in the data, but may include unnecessary predictors. Adjusted RÂ² penalizes extra variables, so a large difference indicates overfitting.\n",
        "\n",
        "22. Why is it important to scale variables in Multiple Linear Regression?\n",
        "    - Scaling ensures variables with different units or ranges contribute equally, improves numerical stability, and helps algorithms (like gradient-based optimizers) converge faster.\n",
        "\n",
        "23. What is polynomial regression?\n",
        "    - Polynomial regression is a type of regression where the relationship between the independent and dependent variable is modeled as a polynomial (e.g., ð‘¦=ð‘Ž+ð‘ð‘¥+ð‘ð‘¥2+â€¦+â€¦) to capture non-linear patterns.\n",
        "\n",
        "24. How does polynomial regression differ from linear regression?\n",
        "    - Linear regression models a straight-line relationship between variables, while polynomial regression models a curved (non-linear) relationship using higher-degree terms of the predictors.\n",
        "\n",
        "25. When is polynomial regression used?\n",
        "    - Polynomial regression is used when the relationship between independent and dependent variables is non-linear and cannot be captured well by a straight line.\n",
        "\n",
        "26. What is the general equation for polynomial regression?\n",
        "    - The general equation is: y=Î²0â€‹+Î²1â€‹x+Î²2â€‹x2+Î²3â€‹x3+â‹¯+Î²nâ€‹xn+Îµ\n",
        "    - where ð‘› n is the degree of the polynomial, ð›½ Î² are coefficients, and ðœ€ Îµ is the error term.\n",
        "\n",
        "27. Can polynomial regression be applied to multiple variables?\n",
        "    - Yes, it can. For multiple variables, polynomial terms of each predictor and their interactions can be included to model non-linear relationships.\n",
        "\n",
        "28. What are the limitations of polynomial regression?\n",
        "    - Limitations of polynomial regression:\n",
        "    1. Prone to overfitting with high-degree polynomials.\n",
        "    2. Sensitive to outliers.\n",
        "    3. Hard to interpret coefficients for higher degrees.\n",
        "    4. Can cause numerical instability for large datasets.\n",
        "\n",
        "29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "    - Methods to evaluate model fit:\n",
        "    1. RÂ² and Adjusted RÂ² â€“ measure explained variance.\n",
        "    2. Mean Squared Error (MSE) / RMSE â€“ measure prediction error.\n",
        "    3. Cross-validation â€“ checks performance on unseen data.\n",
        "    4. Residual plots â€“ inspect patterns for underfitting or overfitting.\n",
        "\n",
        "30. Why is visualization important in polynomial regression?\n",
        "    - Visualization helps to see the non-linear relationship, check model fit, and detect underfitting or overfitting in polynomial regression."
      ],
      "metadata": {
        "id": "oqtQo_vIPHvS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPWiBDy_O0-2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c9584fd-745a-412f-8ed1-16925fb5bcc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Polynomial features transformed (first 5 rows):\n",
            " [[ 1.  1.  1.]\n",
            " [ 1.  2.  4.]\n",
            " [ 1.  3.  9.]\n",
            " [ 1.  4. 16.]\n",
            " [ 1.  5. 25.]]\n",
            "Model coefficients: [0.         0.58106061 0.08712121]\n",
            "Model intercept: 2.650000000000002\n"
          ]
        }
      ],
      "source": [
        "#Q31. How is polynomial regression implemented in Python?\n",
        "#Ans. In Python, polynomial regression is implemented using PolynomialFeatures from sklearn.preprocessing to create polynomial terms, then fitting a LinearRegression model on these transformed features.\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "# Sample data for demonstration\n",
        "X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).reshape(-1, 1) # Independent variable\n",
        "y = np.array([2, 5, 6, 7, 8, 9, 10, 12, 15, 18]) # Dependent variable\n",
        "\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "model = LinearRegression().fit(X_poly, y)\n",
        "\n",
        "print(\"Polynomial features transformed (first 5 rows):\\n\", X_poly[:5])\n",
        "print(\"Model coefficients:\", model.coef_)\n",
        "print(\"Model intercept:\", model.intercept_)"
      ]
    }
  ]
}